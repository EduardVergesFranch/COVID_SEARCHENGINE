{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler Twitter Data\n",
    "\n",
    "This code, will extract tweets from Twitter API and store them to a file.\n",
    "\n",
    "### Important note\n",
    "You will need to create a `config.txt` file in the folder with your Twitter API credentials or introduce them manually in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import Counter\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tokens from file\n",
    "f = open(\"config.txt\", \"r\")\n",
    "tokens = []\n",
    "for line in f:\n",
    "    tokens.append(line.split()[1])\n",
    "\n",
    "## Define tokens for Twitter API\n",
    "access_token1 = tokens[0]\n",
    "access_token_secret1 = tokens[1]\n",
    "\n",
    "consumer_key1 = tokens[2]\n",
    "consumer_secret1 =tokens[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key1, consumer_secret1)\n",
    "auth.set_access_token(access_token1, access_token_secret1)\n",
    "api = API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class MyStreamListener(StreamListener):\n",
    "    \"\"\"\n",
    "    Twitter listener, collects streaming tweets and output to a file\n",
    "    \"\"\"\n",
    "    def __init__(self, api, OUTPUT_FILENAME, stop_condition=10):\n",
    "        \"\"\"\n",
    "        initialize the stream, with num. of tweets and saving the outputfile\n",
    "        \"\"\"\n",
    "        \n",
    "        # this line is needed to import the characteristics of the streaming API\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        \n",
    "        # to-count the number of tweets collected        \n",
    "        self.num_tweets = 0\n",
    "        \n",
    "        # save filename\n",
    "        self.filename = OUTPUT_FILENAME\n",
    "        \n",
    "        # stop-condition\n",
    "        self.stop_condition = stop_condition\n",
    "        \n",
    "\n",
    "    def on_status(self, status):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function runs each time a new bunch of tweets is retrived from the streaming\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(self.filename, \"a+\") as f:\n",
    "            tweet = status._json\n",
    "            \n",
    "            if tweet['text'].startswith('RT'): # Avoid rts for our scrapping\n",
    "                return True\n",
    "            \n",
    "            f.write(json.dumps(tweet) + '\\n')\n",
    "            self.num_tweets += 1\n",
    "            \n",
    "            if self.num_tweets%1000 == 0:\n",
    "                print(\"We have crawled {} tweets\".format(self.num_tweets))\n",
    "        \n",
    "            # Stop condition        \n",
    "            if self.num_tweets <= self.stop_condition:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "\n",
    "    def on_error(self, status):\n",
    "        \"\"\"\n",
    "        function useful to handle errors. It's possible to personalize it \n",
    "        depending on the way we want to handle errors\n",
    "        \"\"\"\n",
    "        \n",
    "        print(status)\n",
    "        time.sleep(10)\n",
    "        \n",
    "        #returning False in on_error disconnects the stream\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download 100000 tweets (without RTs) related to covid ```[\"coronavirus\", \"covid\"]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "OUTPUT_FILENAME = \"../../data/tweets.json\"\n",
    "stop_condition = 100000\n",
    "\n",
    "l = MyStreamListener(api, OUTPUT_FILENAME, stop_condition)\n",
    "# here we recall the Stream Class from Tweepy to input the authentication info and our personalized listener \n",
    "stream = Stream(auth=api.auth, listener=l)\n",
    "\n",
    "\n",
    "# keywords we may want decide to track \n",
    "TRACKING_KEYWORDS = [\"coronavirus\", \"covid\"]\n",
    "stream.filter(\n",
    "    track=TRACKING_KEYWORDS, \n",
    "    is_async=False, \n",
    "    languages = [\"en\"],\n",
    "    since_id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
